
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[codecarbon INFO @ 08:20:24] Energy consumed for RAM : 0.000021 kWh. RAM Power : 5.120183944702148 W
[codecarbon INFO @ 08:20:24] Energy consumed for all GPUs : 0.000221 kWh. Total GPU Power : 53.102000000000004 W
[codecarbon INFO @ 08:20:24] Energy consumed for all CPUs : 0.000198 kWh. Total CPU Power : 47.5 W
[codecarbon INFO @ 08:20:24] 0.000441 kWh of electricity used since the beginning.
[codecarbon INFO @ 08:20:39] Energy consumed for RAM : 0.000043 kWh. RAM Power : 5.120183944702148 W
[codecarbon INFO @ 08:20:39] Energy consumed for all GPUs : 0.000461 kWh. Total GPU Power : 57.528000000000006 W
[codecarbon INFO @ 08:20:39] Energy consumed for all CPUs : 0.000396 kWh. Total CPU Power : 47.5 W
[codecarbon INFO @ 08:20:39] 0.000899 kWh of electricity used since the beginning.
[codecarbon INFO @ 08:20:54] Energy consumed for RAM : 0.000064 kWh. RAM Power : 5.120183944702148 W
[codecarbon INFO @ 08:20:54] Energy consumed for all GPUs : 0.000685 kWh. Total GPU Power : 53.945 W
[codecarbon INFO @ 08:20:54] Energy consumed for all CPUs : 0.000594 kWh. Total CPU Power : 47.5 W
